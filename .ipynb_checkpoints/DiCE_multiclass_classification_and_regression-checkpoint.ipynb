{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating counterfactuals for multi-class classification and regression models\n",
    "This notebook will demonstrate how the DiCE library can be used for multiclass classification and regression for scikit-learn models.\n",
    "You can use any method (\"random\", \"kdtree\", \"genetic\"), just specific it in the method argument in the initialization step. The rest of the code is completely identical.\n",
    "For demonstration, we will be using the genetic algorithm for CFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dice_ml\n",
    "from dice_ml import Dice\n",
    "\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sklearn's internal datasets to demonstrate DiCE's features in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass classification, we will use sklearn's Iris dataset. This data set consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length. More information at https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"healthcare-dataset-stroke-data.csv\"\n",
    "\n",
    "df_iris = pandas.read_csv(path)\n",
    "simplified_dataset = df_iris.loc[:,['age', 'hypertension', 'heart_disease', 'avg_glucose_level',\n",
    "                              'bmi', 'smoking_status', 'stroke']]\n",
    "\n",
    "simplified_dataset.dropna(subset=['bmi'], inplace=True)\n",
    "df_iris = simplified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4909 entries, 0 to 5109\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   age                4909 non-null   float64\n",
      " 1   hypertension       4909 non-null   int64  \n",
      " 2   heart_disease      4909 non-null   int64  \n",
      " 3   avg_glucose_level  4909 non-null   float64\n",
      " 4   bmi                4909 non-null   float64\n",
      " 5   smoking_status     4909 non-null   object \n",
      " 6   stroke             4909 non-null   int64  \n",
      "dtypes: float64(3), int64(3), object(1)\n",
      "memory usage: 306.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_name = \"stroke\"\n",
    "continuous_features_iris = ['age', 'avg_glucose_level', 'bmi']\n",
    "target = df_iris[outcome_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "datasetX = df_iris.drop(outcome_name, axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(datasetX,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)\n",
    "\n",
    "categorical_features = x_train.columns.difference(continuous_features_iris)\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, continuous_features_iris),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf_iris = Pipeline(steps=[('preprocessor', transformations),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "\n",
    "model_iris = clf_iris.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_iris = dice_ml.Data(dataframe=df_iris,\n",
    "                      continuous_features=continuous_features_iris,\n",
    "                      outcome_name='stroke')\n",
    "\n",
    "# We provide the type of model as a parameter (model_type)\n",
    "m_iris = dice_ml.Model(model=model_iris, backend=\"sklearn\", model_type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_genetic_iris = Dice(d_iris, m_iris, method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, all the target values will lie in the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "query data dimension must match training data dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Single input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query_instances_iris \u001b[38;5;241m=\u001b[39m datasetX[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m genetic_iris \u001b[38;5;241m=\u001b[39m \u001b[43mexp_genetic_iris\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_counterfactuals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_instances_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_CFs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m genetic_iris\u001b[38;5;241m.\u001b[39mvisualize_as_dataframe()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dice_ml\\explainer_interfaces\\explainer_base.py:92\u001b[0m, in \u001b[0;36mExplainerBase.generate_counterfactuals\u001b[1;34m(self, query_instances, total_CFs, desired_class, desired_range, permitted_range, features_to_vary, stopping_threshold, posthoc_sparsity_param, posthoc_sparsity_algorithm, verbose, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_instance \u001b[38;5;129;01min\u001b[39;00m tqdm(query_instances_list):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_interface\u001b[38;5;241m.\u001b[39mset_continuous_feature_indexes(query_instance)\n\u001b[1;32m---> 92\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_counterfactuals(\n\u001b[0;32m     93\u001b[0m         query_instance, total_CFs,\n\u001b[0;32m     94\u001b[0m         desired_class\u001b[38;5;241m=\u001b[39mdesired_class,\n\u001b[0;32m     95\u001b[0m         desired_range\u001b[38;5;241m=\u001b[39mdesired_range,\n\u001b[0;32m     96\u001b[0m         permitted_range\u001b[38;5;241m=\u001b[39mpermitted_range,\n\u001b[0;32m     97\u001b[0m         features_to_vary\u001b[38;5;241m=\u001b[39mfeatures_to_vary,\n\u001b[0;32m     98\u001b[0m         stopping_threshold\u001b[38;5;241m=\u001b[39mstopping_threshold,\n\u001b[0;32m     99\u001b[0m         posthoc_sparsity_param\u001b[38;5;241m=\u001b[39mposthoc_sparsity_param,\n\u001b[0;32m    100\u001b[0m         posthoc_sparsity_algorithm\u001b[38;5;241m=\u001b[39mposthoc_sparsity_algorithm,\n\u001b[0;32m    101\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m     cf_examples_arr\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CounterfactualExplanations(cf_examples_list\u001b[38;5;241m=\u001b[39mcf_examples_arr)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dice_ml\\explainer_interfaces\\dice_KD.py:109\u001b[0m, in \u001b[0;36mDiceKD._generate_counterfactuals\u001b[1;34m(self, query_instance, total_CFs, desired_range, desired_class, features_to_vary, permitted_range, sparsity_weight, feature_weights, stopping_threshold, posthoc_sparsity_param, posthoc_sparsity_algorithm, verbose)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Partitioned dataset and KD Tree for each class (binary) of the dataset\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_with_predictions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKD_tree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_KD_tree(data_df_copy, desired_range, desired_class, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_outcome_name)\n\u001b[1;32m--> 109\u001b[0m query_instance, cfs_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_counterfactuals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mquery_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_instance_orig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mdesired_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mtotal_CFs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_vary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mpermitted_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43msparsity_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mstopping_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mposthoc_sparsity_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mposthoc_sparsity_algorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfs_preds \u001b[38;5;241m=\u001b[39m cfs_preds\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp\u001b[38;5;241m.\u001b[39mCounterfactualExamples(data_interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_interface,\n\u001b[0;32m    122\u001b[0m                                   final_cfs_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_cfs_df,\n\u001b[0;32m    123\u001b[0m                                   test_instance_df\u001b[38;5;241m=\u001b[39mquery_instance,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m                                   desired_class\u001b[38;5;241m=\u001b[39mdesired_class,\n\u001b[0;32m    128\u001b[0m                                   model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_type)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dice_ml\\explainer_interfaces\\dice_KD.py:230\u001b[0m, in \u001b[0;36mDiceKD.find_counterfactuals\u001b[1;34m(self, data_df_copy, query_instance, query_instance_orig, desired_range, desired_class, total_CFs, features_to_vary, permitted_range, sparsity_weight, stopping_threshold, posthoc_sparsity_param, posthoc_sparsity_algorithm, verbose)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m query_instance_df_dummies\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    228\u001b[0m         query_instance_df_dummies[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_cfs, cfs_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvary_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_instance_df_dummies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtotal_CFs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mfeatures_to_vary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mpermitted_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquery_instance_orig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43msparsity_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m total_cfs_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_cfs)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_cfs_found \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# post-hoc operation on continuous features to enhance sparsity - only for public data\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dice_ml\\explainer_interfaces\\dice_KD.py:164\u001b[0m, in \u001b[0;36mDiceKD.vary_valid\u001b[1;34m(self, KD_query_instance, total_CFs, features_to_vary, permitted_range, query_instance, sparsity_weight)\u001b[0m\n\u001b[0;32m    161\u001b[0m cfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKD_tree \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_queries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     KD_tree_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKD_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKD_query_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_queries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     distances \u001b[38;5;241m=\u001b[39m KD_tree_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m     indices \u001b[38;5;241m=\u001b[39m KD_tree_output[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32msklearn\\neighbors\\_binary_tree.pxi:1245\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.query\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: query data dimension must match training data dimension"
     ]
    }
   ],
   "source": [
    "# Single input\n",
    "query_instances_iris = datasetX[2:3]\n",
    "\n",
    "genetic_iris = exp_genetic_iris.generate_counterfactuals(query_instances_iris, total_CFs=7, desired_class=1)\n",
    "genetic_iris.visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple queries can be given as input at once\n",
    "query_instances_iris = x_test[17:19]\n",
    "genetic_iris = exp_genetic_iris.generate_counterfactuals(query_instances_iris, total_CFs=7, desired_class=2)\n",
    "genetic_iris.visualize_as_dataframe(show_only_changes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, we will use sklearn's California Housing dataset. This dataset contains California house prices. More information at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = fetch_california_housing()\n",
    "df_housing = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "df_housing[outcome_name] = pd.Series(housing_data.target)\n",
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features_housing = df_housing.drop(outcome_name, axis=1).columns.tolist()\n",
    "target = df_housing[outcome_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split data into train and test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m datasetX \u001b[38;5;241m=\u001b[39m \u001b[43mdf_housing\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(outcome_name, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(datasetX,\n\u001b[0;32m      4\u001b[0m                                                     target,\n\u001b[0;32m      5\u001b[0m                                                     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference(continuous_features_housing)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_housing' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "datasetX = df_housing.drop(outcome_name, axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(datasetX,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "categorical_features = x_train.columns.difference(continuous_features_housing)\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, continuous_features_housing),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "regr_housing = Pipeline(steps=[('preprocessor', transformations),\n",
    "                               ('regressor', RandomForestRegressor())])\n",
    "model_housing = regr_housing.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_housing = dice_ml.Data(dataframe=df_housing, continuous_features=continuous_features_housing, outcome_name=outcome_name)\n",
    "# We provide the type of model as a parameter (model_type)\n",
    "m_housing = dice_ml.Model(model=model_housing, backend=\"sklearn\", model_type='regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_genetic_housing = Dice(d_housing, m_housing, method=\"genetic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, all the target values will lie in the desired range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple queries can be given as input at once\n",
    "query_instances_housing = x_test[2:4]\n",
    "genetic_housing = exp_genetic_housing.generate_counterfactuals(query_instances_housing,\n",
    "                                                               total_CFs=2,\n",
    "                                                               desired_range=[3.0, 5.0])\n",
    "genetic_housing.visualize_as_dataframe(show_only_changes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
